{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7cfcba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import getpass\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai  import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99e85d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "77eab4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIRECTORY = \"feature_data/\" \n",
    "VECTOR_STORE_PATH = \"feature_vector_store\"  # Save to a separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1441491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Scans a directory, groups files by project, and creates an enriched vector store.\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    print(f\"Scanning for feature data in: '{FEATURES_DIRECTORY}'\")\n",
    "\n",
    "    project_files = defaultdict(dict)\n",
    "    for file_path in glob.glob(os.path.join(FEATURES_DIRECTORY, \"*.jsonl\")):\n",
    "        base_name = os.path.basename(file_path).split('.')[0]\n",
    "        project_name_part = base_name.split('-')[1].strip()\n",
    "        \n",
    "        if 'feature' in base_name:\n",
    "            project_files[project_name_part]['features'] = file_path\n",
    "        elif 'data_dictionary' in base_name:\n",
    "            project_files[project_name_part]['dictionary'] = file_path\n",
    "        elif 'compliance' in base_name:\n",
    "            project_files[project_name_part]['compliance'] = file_path\n",
    "\n",
    "    print(f\"Found data for {len(project_files)} projects.\")\n",
    "\n",
    "    all_docs = []\n",
    "    for project_name, files in project_files.items():\n",
    "        print(f\"  - Processing project: {project_name}\")\n",
    "\n",
    "\n",
    "        project_dictionary = []\n",
    "        if files.get('dictionary'):\n",
    "            with open(files['dictionary'], 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    project_dictionary.append(json.loads(line))\n",
    "\n",
    "        project_compliance = []\n",
    "        if files.get('compliance'):\n",
    "            with open(files['compliance'], 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    project_compliance.append(json.loads(line))\n",
    "        \n",
    "        # Format the context once per project\n",
    "        dict_context = \"\\n\".join([f\"- {item.get('variable_name', '')}: {item.get('variable_description', '')}\" for item in project_dictionary])\n",
    "        comp_context = \"\\n\".join([f\"- {item.get('compliance_title', '')}: {item.get('compliance_description', '')}\" for item in project_compliance])\n",
    "\n",
    "        # Iterate through the features file for this project\n",
    "        if files.get('features'):\n",
    "            with open(files['features'], 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    feature = json.loads(line)\n",
    "                    \n",
    "                    content = (\n",
    "                        f\"**Project:** {feature.get('project_name', 'N/A')}\\n\\n\"\n",
    "                        f\"**Feature Title:** {feature.get('feature_title', 'N/A')}\\n\"\n",
    "                        f\"**Feature Type:** {feature.get('feature_type', 'N/A')}\\n\"\n",
    "                        f\"**Description:**\\n{feature.get('feature_description', '')}\\n\\n\"\n",
    "                        f\"--- Project Data Dictionary ---\\n{dict_context}\\n\\n\"\n",
    "                        f\"--- Project Compliance Rules ---\\n{comp_context}\"\n",
    "                    )\n",
    "                    \n",
    "                    metadata = {\n",
    "                        \"project_name\": feature.get('project_name'),\n",
    "                        \"project_id\": feature.get('project_id'),\n",
    "                        \"feature_id\": feature.get('feature_id'),\n",
    "                        \"feature_title\": feature.get('feature_title'),\n",
    "                        \"source_file\": feature.get('reference_file')\n",
    "                    }\n",
    "                    all_docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents were created. Please check file paths and content.\")\n",
    "        return\n",
    "        \n",
    "\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    \n",
    "    vector_store = None\n",
    "    BATCH_SIZE = 1\n",
    "    DELAY_BETWEEN_BATCHES = 5 # 5 seconds\n",
    "\n",
    "    for i in range(0, len(all_docs), BATCH_SIZE):\n",
    "        batch = all_docs[i:i + BATCH_SIZE]\n",
    "        if vector_store is None:\n",
    "            vector_store = FAISS.from_documents(batch, embeddings)\n",
    "        else:\n",
    "            vector_store.add_documents(batch)\n",
    "        \n",
    "        print(f\"  - Processed batch {i // BATCH_SIZE + 1}. Waiting for {DELAY_BETWEEN_BATCHES} seconds...\")\n",
    "        import time\n",
    "        time.sleep(DELAY_BETWEEN_BATCHES)\n",
    "\n",
    "    vector_store.save_local(VECTOR_STORE_PATH)\n",
    "    print(\"Ingestion complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da050ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for feature data in: 'feature_data/'\n",
      "Found data for 3 projects.\n",
      "  - Processing project: gamma j\n",
      "  - Processing project: library\n",
      "  - Processing project: model manager\n",
      "\n",
      "Created a total of 80 enriched feature documents.\n",
      "Initialized embedding model.\n",
      "Creating vector store with batching to handle rate limits...\n",
      "  - Processed batch 1. Waiting for 5 seconds...\n",
      "  - Processed batch 2. Waiting for 5 seconds...\n",
      "  - Processed batch 3. Waiting for 5 seconds...\n",
      "  - Processed batch 4. Waiting for 5 seconds...\n",
      "  - Processed batch 5. Waiting for 5 seconds...\n",
      "  - Processed batch 6. Waiting for 5 seconds...\n",
      "  - Processed batch 7. Waiting for 5 seconds...\n",
      "  - Processed batch 8. Waiting for 5 seconds...\n",
      "  - Processed batch 9. Waiting for 5 seconds...\n",
      "  - Processed batch 10. Waiting for 5 seconds...\n",
      "  - Processed batch 11. Waiting for 5 seconds...\n",
      "  - Processed batch 12. Waiting for 5 seconds...\n",
      "  - Processed batch 13. Waiting for 5 seconds...\n",
      "  - Processed batch 14. Waiting for 5 seconds...\n",
      "  - Processed batch 15. Waiting for 5 seconds...\n",
      "  - Processed batch 16. Waiting for 5 seconds...\n",
      "  - Processed batch 17. Waiting for 5 seconds...\n",
      "  - Processed batch 18. Waiting for 5 seconds...\n",
      "  - Processed batch 19. Waiting for 5 seconds...\n",
      "  - Processed batch 20. Waiting for 5 seconds...\n",
      "  - Processed batch 21. Waiting for 5 seconds...\n",
      "  - Processed batch 22. Waiting for 5 seconds...\n",
      "  - Processed batch 23. Waiting for 5 seconds...\n",
      "  - Processed batch 24. Waiting for 5 seconds...\n",
      "  - Processed batch 25. Waiting for 5 seconds...\n",
      "  - Processed batch 26. Waiting for 5 seconds...\n",
      "  - Processed batch 27. Waiting for 5 seconds...\n",
      "  - Processed batch 28. Waiting for 5 seconds...\n",
      "  - Processed batch 29. Waiting for 5 seconds...\n",
      "  - Processed batch 30. Waiting for 5 seconds...\n",
      "  - Processed batch 31. Waiting for 5 seconds...\n",
      "  - Processed batch 32. Waiting for 5 seconds...\n",
      "  - Processed batch 33. Waiting for 5 seconds...\n",
      "  - Processed batch 34. Waiting for 5 seconds...\n",
      "  - Processed batch 35. Waiting for 5 seconds...\n",
      "  - Processed batch 36. Waiting for 5 seconds...\n",
      "  - Processed batch 37. Waiting for 5 seconds...\n",
      "  - Processed batch 38. Waiting for 5 seconds...\n",
      "  - Processed batch 39. Waiting for 5 seconds...\n",
      "  - Processed batch 40. Waiting for 5 seconds...\n",
      "  - Processed batch 41. Waiting for 5 seconds...\n",
      "  - Processed batch 42. Waiting for 5 seconds...\n",
      "  - Processed batch 43. Waiting for 5 seconds...\n",
      "  - Processed batch 44. Waiting for 5 seconds...\n",
      "  - Processed batch 45. Waiting for 5 seconds...\n",
      "  - Processed batch 46. Waiting for 5 seconds...\n",
      "  - Processed batch 47. Waiting for 5 seconds...\n",
      "  - Processed batch 48. Waiting for 5 seconds...\n",
      "  - Processed batch 49. Waiting for 5 seconds...\n",
      "  - Processed batch 50. Waiting for 5 seconds...\n",
      "  - Processed batch 51. Waiting for 5 seconds...\n",
      "  - Processed batch 52. Waiting for 5 seconds...\n",
      "  - Processed batch 53. Waiting for 5 seconds...\n",
      "  - Processed batch 54. Waiting for 5 seconds...\n",
      "  - Processed batch 55. Waiting for 5 seconds...\n",
      "  - Processed batch 56. Waiting for 5 seconds...\n",
      "  - Processed batch 57. Waiting for 5 seconds...\n",
      "  - Processed batch 58. Waiting for 5 seconds...\n",
      "  - Processed batch 59. Waiting for 5 seconds...\n",
      "  - Processed batch 60. Waiting for 5 seconds...\n",
      "  - Processed batch 61. Waiting for 5 seconds...\n",
      "  - Processed batch 62. Waiting for 5 seconds...\n",
      "  - Processed batch 63. Waiting for 5 seconds...\n",
      "  - Processed batch 64. Waiting for 5 seconds...\n",
      "  - Processed batch 65. Waiting for 5 seconds...\n",
      "  - Processed batch 66. Waiting for 5 seconds...\n",
      "  - Processed batch 67. Waiting for 5 seconds...\n",
      "  - Processed batch 68. Waiting for 5 seconds...\n",
      "  - Processed batch 69. Waiting for 5 seconds...\n",
      "  - Processed batch 70. Waiting for 5 seconds...\n",
      "  - Processed batch 71. Waiting for 5 seconds...\n",
      "  - Processed batch 72. Waiting for 5 seconds...\n",
      "  - Processed batch 73. Waiting for 5 seconds...\n",
      "  - Processed batch 74. Waiting for 5 seconds...\n",
      "  - Processed batch 75. Waiting for 5 seconds...\n",
      "  - Processed batch 76. Waiting for 5 seconds...\n",
      "  - Processed batch 77. Waiting for 5 seconds...\n",
      "  - Processed batch 78. Waiting for 5 seconds...\n",
      "  - Processed batch 79. Waiting for 5 seconds...\n",
      "  - Processed batch 80. Waiting for 5 seconds...\n",
      "✅ Ingestion complete. Enriched vector store saved successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
